{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c8ebfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from typing import TypedDict\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a4cada8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatGoogleGenerativeAI(model= \"models/gemini-2.0-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3623a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlogSchema(TypedDict):\n",
    "    topic : str\n",
    "    outlines: str\n",
    "    blog: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99a31719",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outline_llm(state: BlogSchema) -> BlogSchema:\n",
    "    topic = state['topic']\n",
    "    prompt = PromptTemplate(\n",
    "        template = 'For the topic {topic} generate detailed outline',\n",
    "        input_variables = ['topic']\n",
    "    )\n",
    "    chain = prompt | model | StrOutputParser()\n",
    "    outline = chain.invoke({\"topic\": topic})\n",
    "    \n",
    "    return {\"outlines\": outline}\n",
    "\n",
    "def blog_llm(state: BlogSchema) -> BlogSchema:\n",
    "    outlines = state['outlines']\n",
    "    prompt = PromptTemplate(\n",
    "        template = 'create a full flegded blog from the outlines \\n {outline}',\n",
    "        input_variables = ['outline']\n",
    "    )\n",
    "    chain = prompt | model | StrOutputParser()\n",
    "    blog = chain.invoke({\"outline\": outlines})\n",
    "    \n",
    "    return {\"blog\": blog}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0ad3c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(state_schema=BlogSchema)\n",
    "\n",
    "graph.add_node('outline_llm', outline_llm)\n",
    "graph.add_node('blog_llm', blog_llm)\n",
    "\n",
    "graph.add_edge(START, 'outline_llm')\n",
    "graph.add_edge('outline_llm', 'blog_llm')\n",
    "graph.add_edge('blog_llm', END)\n",
    "\n",
    "workflow = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6920d9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_state = workflow.invoke({\"topic\": \"Data Science\", \"outlines\": \"\",\"blog\": \"\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "216371d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Data Science: Detailed Outline\n",
      "\n",
      "This outline provides a comprehensive overview of Data Science, covering its core concepts, methodologies, tools, and applications.\n",
      "\n",
      "**I. Introduction to Data Science**\n",
      "\n",
      "   A. **What is Data Science?**\n",
      "      1. Definition and scope of Data Science\n",
      "      2. Distinguishing Data Science from related fields (e.g., Statistics, Machine Learning, Business Intelligence)\n",
      "      3. The Data Science process: From data to insights to action\n",
      "   B. **The Data Science Ecosystem**\n",
      "      1. Key players: Data Scientists, Data Engineers, Business Analysts, Domain Experts\n",
      "      2. Roles and responsibilities within a Data Science team\n",
      "      3. Importance of collaboration and communication\n",
      "   C. **Applications of Data Science**\n",
      "      1. Examples across various industries (e.g., healthcare, finance, retail, marketing)\n",
      "      2. Real-world use cases: Fraud detection, personalized recommendations, predictive maintenance, etc.\n",
      "   D. **Ethical Considerations in Data Science**\n",
      "      1. Bias in data and algorithms\n",
      "      2. Privacy and security concerns\n",
      "      3. Responsible use of data and insights\n",
      "   E. **The Future of Data Science**\n",
      "      1. Emerging trends and technologies (e.g., Explainable AI, Federated Learning, AutoML)\n",
      "      2. The evolving role of the Data Scientist\n",
      "\n",
      "**II. Data Acquisition and Management**\n",
      "\n",
      "   A. **Data Sources**\n",
      "      1. Internal data: Databases, CRM systems, logs, sensors\n",
      "      2. External data: Public datasets, APIs, web scraping, social media data\n",
      "      3. Data formats: Structured (SQL), semi-structured (JSON, XML), unstructured (text, images, audio)\n",
      "   B. **Data Extraction and Transformation (ETL)**\n",
      "      1. Data extraction techniques: Connecting to databases, APIs, web scraping\n",
      "      2. Data cleaning and preprocessing: Handling missing values, outliers, inconsistencies\n",
      "      3. Data transformation: Normalization, standardization, feature engineering\n",
      "   C. **Data Storage and Management**\n",
      "      1. Relational databases (SQL): MySQL, PostgreSQL, Oracle\n",
      "      2. NoSQL databases: MongoDB, Cassandra, Redis\n",
      "      3. Data warehouses: Snowflake, Amazon Redshift, Google BigQuery\n",
      "      4. Data lakes: Hadoop, Amazon S3, Azure Data Lake Storage\n",
      "   D. **Data Governance and Security**\n",
      "      1. Data quality management\n",
      "      2. Data lineage and auditability\n",
      "      3. Access control and data encryption\n",
      "   E. **Big Data Technologies**\n",
      "      1. Hadoop ecosystem: HDFS, MapReduce, YARN\n",
      "      2. Spark: Core, SQL, Streaming, MLlib, GraphX\n",
      "      3. Cloud-based data platforms: AWS, Azure, GCP\n",
      "\n",
      "**III. Data Exploration and Visualization**\n",
      "\n",
      "   A. **Exploratory Data Analysis (EDA)**\n",
      "      1. Descriptive statistics: Mean, median, standard deviation, quartiles\n",
      "      2. Data visualization: Histograms, scatter plots, box plots, heatmaps\n",
      "      3. Correlation analysis: Identifying relationships between variables\n",
      "      4. Hypothesis testing: Formulating and testing statistical hypotheses\n",
      "   B. **Data Visualization Tools**\n",
      "      1. Python libraries: Matplotlib, Seaborn, Plotly\n",
      "      2. R libraries: ggplot2, Lattice\n",
      "      3. Business intelligence tools: Tableau, Power BI, Qlik\n",
      "   C. **Data Storytelling**\n",
      "      1. Communicating insights effectively through visualizations and narratives\n",
      "      2. Tailoring visualizations to different audiences\n",
      "      3. Creating interactive dashboards and reports\n",
      "   D. **Dimensionality Reduction Techniques**\n",
      "      1. Principal Component Analysis (PCA)\n",
      "      2. t-distributed Stochastic Neighbor Embedding (t-SNE)\n",
      "      3. Feature selection methods\n",
      "\n",
      "**IV. Machine Learning**\n",
      "\n",
      "   A. **Fundamentals of Machine Learning**\n",
      "      1. Supervised learning: Regression, classification\n",
      "      2. Unsupervised learning: Clustering, dimensionality reduction, association rule mining\n",
      "      3. Reinforcement learning: Agents, environments, rewards, policies\n",
      "      4. Model evaluation: Metrics for different types of models\n",
      "   B. **Supervised Learning Algorithms**\n",
      "      1. Linear Regression and Logistic Regression\n",
      "      2. Decision Trees and Random Forests\n",
      "      3. Support Vector Machines (SVM)\n",
      "      4. K-Nearest Neighbors (KNN)\n",
      "      5. Naive Bayes\n",
      "   C. **Unsupervised Learning Algorithms**\n",
      "      1. K-Means Clustering\n",
      "      2. Hierarchical Clustering\n",
      "      3. DBSCAN\n",
      "      4. Association Rule Mining (Apriori, Eclat)\n",
      "   D. **Model Selection and Evaluation**\n",
      "      1. Bias-variance tradeoff\n",
      "      2. Cross-validation techniques: K-fold cross-validation, stratified cross-validation\n",
      "      3. Hyperparameter tuning: Grid search, random search, Bayesian optimization\n",
      "      4. Model evaluation metrics: Accuracy, precision, recall, F1-score, AUC-ROC, RMSE\n",
      "   E. **Deep Learning**\n",
      "      1. Neural Networks: Architectures, activation functions, backpropagation\n",
      "      2. Convolutional Neural Networks (CNNs): Image recognition, object detection\n",
      "      3. Recurrent Neural Networks (RNNs): Natural language processing, time series analysis\n",
      "      4. Deep Learning Frameworks: TensorFlow, PyTorch, Keras\n",
      "\n",
      "**V. Statistical Modeling and Inference**\n",
      "\n",
      "   A. **Probability and Distributions**\n",
      "      1. Probability theory: Basic concepts, conditional probability, Bayes' theorem\n",
      "      2. Common probability distributions: Normal, binomial, Poisson, exponential\n",
      "   B. **Statistical Inference**\n",
      "      1. Hypothesis testing: Null hypothesis, alternative hypothesis, p-value\n",
      "      2. Confidence intervals: Estimating population parameters\n",
      "      3. A/B testing: Comparing different versions of a product or service\n",
      "   C. **Regression Analysis**\n",
      "      1. Linear regression: Assumptions, model fitting, interpretation of coefficients\n",
      "      2. Multiple regression: Handling multiple predictor variables\n",
      "      3. Logistic regression: Modeling binary outcomes\n",
      "   D. **Time Series Analysis**\n",
      "      1. Time series decomposition: Trend, seasonality, cyclicality, randomness\n",
      "      2. Forecasting methods: ARIMA, Exponential Smoothing\n",
      "   E. **Causal Inference**\n",
      "      1. Observational studies vs. experimental studies\n",
      "      2. Confounding variables and methods for controlling them\n",
      "      3. Potential outcomes framework\n",
      "      4. Instrumental variables, regression discontinuity\n",
      "\n",
      "**VI. Natural Language Processing (NLP)**\n",
      "\n",
      "   A. **Fundamentals of NLP**\n",
      "      1. Text preprocessing: Tokenization, stemming, lemmatization\n",
      "      2. Feature extraction: Bag of Words, TF-IDF, word embeddings\n",
      "      3. Language modeling: N-grams, neural language models\n",
      "   B. **NLP Tasks**\n",
      "      1. Text classification: Sentiment analysis, spam detection\n",
      "      2. Named Entity Recognition (NER)\n",
      "      3. Machine Translation\n",
      "      4. Question Answering\n",
      "      5. Text Summarization\n",
      "   C. **NLP Tools and Libraries**\n",
      "      1. NLTK\n",
      "      2. spaCy\n",
      "      3. Transformers (Hugging Face)\n",
      "   D. **Advanced NLP Techniques**\n",
      "      1. Deep learning for NLP: RNNs, LSTMs, Transformers\n",
      "      2. Fine-tuning pre-trained language models\n",
      "\n",
      "**VII. Data Science Tools and Technologies**\n",
      "\n",
      "   A. **Programming Languages**\n",
      "      1. Python: Libraries for data manipulation, analysis, visualization, and machine learning (Pandas, NumPy, Scikit-learn, Matplotlib, Seaborn)\n",
      "      2. R: Statistical computing and graphics\n",
      "      3. SQL: Database querying and manipulation\n",
      "   B. **Integrated Development Environments (IDEs)**\n",
      "      1. Jupyter Notebook/Lab\n",
      "      2. VS Code\n",
      "      3. PyCharm\n",
      "      4. RStudio\n",
      "   C. **Version Control**\n",
      "      1. Git and GitHub\n",
      "   D. **Cloud Computing Platforms**\n",
      "      1. Amazon Web Services (AWS)\n",
      "      2. Microsoft Azure\n",
      "      3. Google Cloud Platform (GCP)\n",
      "   E. **Data Science Platforms**\n",
      "      1. Dataiku\n",
      "      2. KNIME\n",
      "      3. RapidMiner\n",
      "\n",
      "**VIII. Data Science Project Management**\n",
      "\n",
      "   A. **Project Lifecycle**\n",
      "      1. Defining the problem and objectives\n",
      "      2. Data collection and preparation\n",
      "      3. Model development and evaluation\n",
      "      4. Deployment and monitoring\n",
      "   B. **Agile Methodologies**\n",
      "      1. Scrum\n",
      "      2. Kanban\n",
      "   C. **Communication and Collaboration**\n",
      "      1. Effective communication with stakeholders\n",
      "      2. Collaboration tools: Slack, Microsoft Teams\n",
      "   D. **Documentation**\n",
      "      1. Code documentation\n",
      "      2. Project reports\n",
      "      3. Model documentation\n",
      "\n",
      "**IX. Deploying Data Science Solutions**\n",
      "\n",
      "   A. **Model Deployment Strategies**\n",
      "      1. Batch processing\n",
      "      2. Real-time prediction\n",
      "      3. A/B testing\n",
      "   B. **Deployment Platforms**\n",
      "      1. Cloud platforms: AWS SageMaker, Azure Machine Learning, Google AI Platform\n",
      "      2. Containerization: Docker, Kubernetes\n",
      "      3. Serverless computing: AWS Lambda, Azure Functions, Google Cloud Functions\n",
      "   C. **Model Monitoring and Maintenance**\n",
      "      1. Monitoring model performance\n",
      "      2. Retraining models\n",
      "      3. Versioning models\n",
      "   D. **API Development**\n",
      "      1. REST APIs\n",
      "      2. Flask, FastAPI\n",
      "\n",
      "**X. Case Studies and Real-World Applications**\n",
      "\n",
      "   A. **Detailed analysis of successful Data Science projects in various industries:**\n",
      "      1. Healthcare: Predictive diagnostics, drug discovery\n",
      "      2. Finance: Fraud detection, risk management\n",
      "      3. Retail: Personalized recommendations, inventory optimization\n",
      "      4. Marketing: Customer segmentation, targeted advertising\n",
      "      5. Manufacturing: Predictive maintenance, quality control\n",
      "   B. **Lessons learned and best practices from these case studies.**\n",
      "\n",
      "This outline provides a solid foundation for understanding and exploring the field of Data Science. Remember to delve deeper into each topic to gain a more comprehensive understanding. Good luck!\n"
     ]
    }
   ],
   "source": [
    "print(final_state['outlines'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c601904d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
